{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"LLORMA-T.ipynb","provenance":[],"collapsed_sections":[],"authorship_tag":"ABX9TyM+SVyprP43ZUh0PcHI18oc"},"kernelspec":{"name":"python3","display_name":"Python 3"},"accelerator":"GPU"},"cells":[{"cell_type":"markdown","metadata":{"id":"M8Ycxgv9Yog6","colab_type":"text"},"source":["configs.py"]},{"cell_type":"code","metadata":{"id":"Ev7YcHvaYff0","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600754196788,"user_tz":240,"elapsed":768,"user":{"displayName":"Haiqi Zhu","photoUrl":"","userId":"00612599187227358197"}}},"source":["\n","GPU_MEMORY_FRAC = 0.95\n","N_SHOT = 0\n","\n","N_ANCHOR = 100\n","\n","PRE_RANK = 5\n","PRE_LEARNING_RATE = 2e-4\n","PRE_LAMBDA = 10\n","\n","RANK = 20\n","LEARNING_RATE = 1e-2\n","LAMBDA = 1e-3\n","BATCH_SIZE = 1000\n","\n","USE_CACHE = True"],"execution_count":2,"outputs":[]},{"cell_type":"code","metadata":{"id":"702-Pf6iYVfI","colab_type":"code","colab":{}},"source":["anchor.py "],"execution_count":null,"outputs":[]},{"cell_type":"code","metadata":{"id":"H5hNPlIaX7zl","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600754205745,"user_tz":240,"elapsed":1124,"user":{"displayName":"Haiqi Zhu","photoUrl":"","userId":"00612599187227358197"}}},"source":["import random\n","\n","import numpy as np\n","from sklearn.preprocessing import normalize\n","\n","\n","\n","\n","def _init_anchor_points(train_data, row_k, col_k):\n","    train_user_ids = train_data[:, 0].astype(np.int64)\n","    train_item_ids = train_data[:, 1].astype(np.int64)\n","\n","    anchor_idxs = []\n","    while len(anchor_idxs) < N_ANCHOR:\n","        anchor_idx = random.randint(0, train_data.shape[0] - 1)\n","        if anchor_idx in anchor_idxs:\n","            continue\n","\n","        anchor_row = train_data[anchor_idx]\n","        user_id = int(anchor_row[0])\n","        item_id = int(anchor_row[1])\n","\n","        k = np.multiply(row_k[user_id][train_user_ids],\n","                        col_k[item_id][train_item_ids])\n","        sum_a_of_anchor = np.sum(k)\n","        if sum_a_of_anchor < 1:\n","            continue\n","\n","        print('>> %10d\\t%d' % (anchor_idx, sum_a_of_anchor))\n","        anchor_idxs.append(anchor_idx)\n","\n","    return anchor_idxs\n","\n","\n","def _get_distance_matrix(latent):\n","    _normalized_latent = normalize(latent, axis=1)\n","    # print(_normalized_latent.shape)\n","\n","    cos = np.matmul(_normalized_latent, _normalized_latent.T)\n","    cos = np.clip(cos, -1, 1)\n","    d = np.arccos(cos)\n","    assert np.count_nonzero(np.isnan(d)) == 0\n","    return d\n","\n","\n","def _get_k_from_distance(d):\n","    m = np.zeros(d.shape)\n","    m[d < 0.8] = 1\n","    return np.multiply(np.subtract(np.ones(d.shape), np.square(d)), m)\n","\n","\n","def _get_ks_from_latents(row_latent, col_latent):\n","\n","    # for i in range(row_latent.shape[0]):\n","    #     print(row_latent[i][:4])\n","    #\n","    # assert False\n","    row_d = _get_distance_matrix(row_latent)\n","    col_d = _get_distance_matrix(col_latent)\n","\n","    row_k = _get_k_from_distance(row_d)\n","    col_k = _get_k_from_distance(col_d)\n","\n","    return row_k, col_k\n","\n","\n","class AnchorManager:\n","    def __init__(\n","            self,\n","            session,\n","            models,\n","            batch_manager,\n","            row_latent_init,\n","            col_latent_init, ):\n","\n","        train_data = batch_manager.train_data\n","\n","        row_latent = row_latent_init\n","        col_latent = col_latent_init\n","\n","        row_k, col_k = _get_ks_from_latents(row_latent, col_latent)\n","\n","        anchor_idxs = _init_anchor_points(train_data, row_k, col_k)\n","        assert len(anchor_idxs) == N_ANCHOR\n","        # print(anchor_idxs)\n","        anchor_points = train_data[anchor_idxs]\n","\n","        self.train_data = train_data\n","        self.valid_data = batch_manager.valid_data\n","        self.test_data = batch_manager.test_data\n","\n","        self.anchor_idxs = anchor_idxs\n","        self.anchor_points = anchor_points\n","\n","        self.row_k = row_k\n","        self.col_k = col_k\n","\n","    def _get_k(self, anchor_idx, data):\n","        row_k = self.row_k\n","        col_k = self.col_k\n","        anchor_point = self.anchor_points[anchor_idx]\n","\n","        user_id = int(anchor_point[0])\n","        item_id = int(anchor_point[1])\n","\n","        user_ids = data[:, 0].astype(np.int64)\n","        item_ids = data[:, 1].astype(np.int64)\n","\n","        return np.multiply(row_k[user_id][user_ids], col_k[item_id][item_ids])\n","\n","    def get_train_k(self, anchor_idx):\n","        return self._get_k(anchor_idx, self.train_data)\n","\n","    def get_valid_k(self, anchor_idx):\n","        return self._get_k(anchor_idx, self.valid_data)\n","\n","    def get_test_k(self, anchor_idx):\n","        return self._get_k(anchor_idx, self.test_data)\n"],"execution_count":3,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"m2MN4s-jZAv6","colab_type":"text"},"source":["base-dataset"]},{"cell_type":"code","metadata":{"id":"GNa9QGoaZDDg","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600754309970,"user_tz":240,"elapsed":1013,"user":{"displayName":"Haiqi Zhu","photoUrl":"","userId":"00612599187227358197"}}},"source":["import os\n","import random\n","\n","import numpy as np\n","\n","# from ..configs import *\n","\n","\n","def _make_dir_if_not_exists(path):\n","    if not os.path.exists(path):\n","        os.mkdir(path)\n","\n","\n","class DatasetManager:\n","    KIND_MOVIELENS_100K = 'movielens-100k'\n","    KIND_MOVIELENS_1M = 'movielens-1m'\n","    KIND_MOVIELENS_10M = 'movielens-10m'\n","    KIND_MOVIELENS_20M = 'movielens-20m'\n","    KIND_NETFLIX = 'netflix'\n","\n","    KIND_OBJECTS = ( \\\n","        (KIND_MOVIELENS_100K, 'http://files.grouplens.org/datasets/movielens/ml-100k.zip'), \\\n","        (KIND_MOVIELENS_1M,  'http://files.grouplens.org/datasets/movielens/ml-1m.zip'), \\\n","        (KIND_MOVIELENS_10M, 'http://files.grouplens.org/datasets/movielens/ml-10m.zip'), \\\n","        (KIND_MOVIELENS_20M, 'http://files.grouplens.org/datasets/movielens/ml-20m.zip'), \\\n","        (KIND_NETFLIX, None)\n","    )\n","\n","    def _set_kind_and_url(self, kind):\n","        self.kind = kind\n","        for k, url in self.KIND_OBJECTS:\n","            if k == kind:\n","                self.url = url\n","                return True\n","        raise NotImplementedError()\n","\n","    def _download_data_if_not_exists(self):\n","        if not os.path.exists('data/{}'.format(self.kind)):\n","            os.system('wget {url} -O data/{kind}.zip'.format(\n","                url=self.url, kind=self.kind))\n","            os.system(\n","                'unzip data/{kind}.zip -d data/{kind}/'.format(kind=self.kind))\n","\n","    def __init_data(self, detail_path, delimiter, header=False):\n","        current_u = 0\n","        u_dict = {}\n","        current_i = 0\n","        i_dict = {}\n","\n","        data = []\n","        with open('data/{}{}'.format(self.kind, detail_path), 'r') as f:\n","            if header:\n","                f.readline()\n","\n","            for line in f:\n","                cols = line.strip().split(delimiter)\n","                assert len(cols) == 4\n","                # cols = [float(c) for c in cols]\n","                user_id = cols[0]\n","                item_id = cols[1]\n","                r = float(cols[2])\n","                t = int(cols[3])\n","\n","                u = u_dict.get(user_id, None)\n","                if u is None:\n","                    u_dict[user_id] = current_u\n","                    u = current_u\n","                    current_u += 1\n","\n","                i = i_dict.get(item_id, None)\n","                if i is None:\n","                    # print(current_i)\n","                    i_dict[item_id] = current_i\n","                    i = current_i\n","                    current_i += 1\n","\n","                data.append((u, i, r, t))\n","            f.close()\n","\n","        data = np.array(data)\n","        np.save('data/{}/data.npy'.format(self.kind), data)\n","\n","    def _init_data(self):\n","        if self.kind == self.KIND_MOVIELENS_100K:\n","            self.__init_data('/ml-100k/u.data', '\\t')\n","        elif self.kind == self.KIND_MOVIELENS_1M:\n","            self.__init_data('/ml-1m/ratings.dat', '::')\n","        elif self.kind == self.KIND_MOVIELENS_10M:\n","            self.__init_data('/ml-10M100K/ratings.dat', '::')\n","        elif self.kind == self.KIND_MOVIELENS_20M:\n","            self.__init_data('/ml-20m/ratings.csv', ',', header=True)\n","        else:\n","            raise NotImplementedError()\n","\n","    def _load_base_data(self):\n","        return np.load('data/{}/data.npy'.format(self.kind))\n","\n","    def _split_data(self):\n","        data = self.data\n","        n_shot = self.n_shot\n","        np.random.shuffle(data)\n","\n","        if self.n_shot == -1:\n","            # n_shot이 -1일때는 더 sparse하게 전체 레이팅을 9:1로 test train set을 나눈다.\n","            n_train = int(data.shape[0] * 0.1)\n","            n_valid = int(n_train * 0.9)\n","\n","            train_data = data[:n_valid]\n","            valid_data = data[n_valid:n_train]\n","            test_data = data[n_train:]\n","\n","            np.save(self._get_npy_path('train'), train_data)\n","            np.save(self._get_npy_path('valid'), valid_data)\n","            np.save(self._get_npy_path('test'), test_data)\n","\n","        elif self.n_shot == 0:\n","            # n_shot이 0일때는 다른 알고리즘들처럼 전체 레이팅을 1:9로 test train set을 나눈다.\n","            n_train = int(data.shape[0] * 0.9)\n","            n_valid = int(n_train * 0.98)\n","\n","            train_data = data[:n_valid]\n","            valid_data = data[n_valid:n_train]\n","            test_data = data[n_train:]\n","\n","            np.save(self._get_npy_path('train'), train_data)\n","            np.save(self._get_npy_path('valid'), valid_data)\n","            np.save(self._get_npy_path('test'), test_data)\n","\n","        else:\n","            # 전체 유저 중에 20%를 일단 test user로 뗍니다.\n","            test_user_ids = random.sample(\n","                list(range(self.n_user)), self.n_user // 5)\n","\n","            train_data = []\n","            test_data = []\n","            count_dict = {}\n","            for i in range(data.shape[0]):\n","                row = data[i]\n","                user_id = int(row[0])\n","                if user_id in test_user_ids:\n","                    count = count_dict.get(user_id, 0)\n","                    if count < n_shot:\n","                        train_data.append(row)\n","                    else:\n","                        test_data.append(row)\n","                    count_dict[user_id] = count + 1\n","                else:\n","                    train_data.append(row)\n","\n","            train_data = np.array(train_data)\n","            n_valid = int(train_data.shape[0] * 0.98)\n","            train_data, valid_data = train_data[:n_valid], train_data[n_valid:]\n","\n","            np.save(self._get_npy_path('train'), train_data)\n","            np.save(self._get_npy_path('valid'), valid_data)\n","\n","            test_data = np.array(test_data)\n","            np.save(self._get_npy_path('test'), test_data)\n","\n","    def _get_npy_path(self, split_kind):\n","        return 'data/{}/shot-{}/{}.npy'.format(self.kind, self.n_shot,\n","                                               split_kind)\n","\n","    def __init__(self, kind, n_shot=0):\n","        assert type(n_shot) == int and n_shot >= -1\n","\n","        _make_dir_if_not_exists('data')\n","        self._set_kind_and_url(kind)\n","        self._download_data_if_not_exists()\n","        self.n_shot = n_shot\n","\n","        # 예쁜 형태로 정제된 npy 파일이 없으면, 정제를 수행합니다.\n","        if not os.path.exists('data/{}/data.npy'.format(kind)):\n","            self._init_data()\n","        self.data = self._load_base_data()\n","\n","        _make_dir_if_not_exists(\n","            'data/{}/shot-{}'.format(self.kind, self.n_shot))\n","\n","        self.n_user = int(np.max(self.data[:, 0])) + 1\n","        self.n_item = int(np.max(self.data[:, 1])) + 1\n","        self.n_row = self.n_user\n","        self.n_col = self.n_item\n","\n","        # split된 데이터가 없으면 split합니다.\n","        if not os.path.exists(\n","                self._get_npy_path('train')) or not os.path.exists(\n","                    self._get_npy_path('valid')) or not os.path.exists(\n","                        self._get_npy_path('test')):\n","            self._split_data()\n","\n","        self.train_data = np.load(self._get_npy_path('train'))\n","        self.valid_data = np.load(self._get_npy_path('valid'))\n","        self.test_data = np.load(self._get_npy_path('test'))\n","\n","    def get_train_data(self):\n","        return self.train_data\n","\n","    def get_valid_data(self):\n","        return self.valid_data\n","\n","    def get_test_data(self):\n","        return self.test_data\n","\n","\n","# if __name__ == '__main__':\n","#     kind = DatasetManager.KIND_MOVIELENS_100K\n","#     kind = DatasetManager.KIND_MOVIELENS_1M\n","#     kind = DatasetManager.KIND_MOVIELENS_10M\n","#     kind = DatasetManager.KIND_MOVIELENS_20M\n","#     dataset_manager = DatasetManager(kind)"],"execution_count":5,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"p91pOQs8Yt3U","colab_type":"text"},"source":["batch.py"]},{"cell_type":"code","metadata":{"id":"GtzsX6LNYufx","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600754325628,"user_tz":240,"elapsed":780,"user":{"displayName":"Haiqi Zhu","photoUrl":"","userId":"00612599187227358197"}}},"source":["import numpy as np\n","\n","\n","\n","class BatchManager:\n","    def __init__(self, kind):\n","        self.kind = kind\n","        dataset_manager = DatasetManager(kind, N_SHOT)\n","        self.train_data = dataset_manager.get_train_data()\n","        self.valid_data = dataset_manager.get_valid_data()\n","        self.test_data = dataset_manager.get_test_data()\n","\n","        self.n_user = int(\n","            max(\n","                np.max(self.train_data[:, 0]),\n","                np.max(self.valid_data[:, 0]), np.max(self.test_data[:,\n","                                                                     0]))) + 1\n","        self.n_item = int(\n","            max(\n","                np.max(self.train_data[:, 1]),\n","                np.max(self.valid_data[:, 1]), np.max(self.test_data[:,\n","                                                                     1]))) + 1\n","        self.mu = np.mean(self.train_data[:, 2])\n","        self.std = np.std(self.train_data[:, 2])"],"execution_count":7,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"9gh247rDZisX","colab_type":"text"},"source":["base/rprop.py "]},{"cell_type":"code","metadata":{"id":"wjJttGw9Zjqs","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600754450743,"user_tz":240,"elapsed":2359,"user":{"displayName":"Haiqi Zhu","photoUrl":"","userId":"00612599187227358197"}}},"source":["\"\"\"\n","    RProp (Resilient Backpropagation) for TensorFlow.\n","    This code is forked form \"https://raw.githubusercontent.com/dirkweissenborn/genie-kb/master/rprop.py\".\n","\"\"\"\n","\n","from __future__ import absolute_import\n","from __future__ import division\n","from __future__ import print_function\n","\n","import tensorflow as tf\n","from tensorflow.python.framework import ops\n","from tensorflow.python.training import optimizer\n","\n","\n","class RPropOptimizer(optimizer.Optimizer):\n","    \"\"\"\n","        Optimizer that implements the RProp algorithm.\n","    \"\"\"\n","\n","    def __init__(self,\n","                 stepsize=0.1,\n","                 etaplus=1.2,\n","                 etaminus=0.5,\n","                 stepsizemax=50.0,\n","                 stepsizemin=1e-6,\n","                 use_locking=False,\n","                 name=\"RProp\"):\n","        super(RPropOptimizer, self).__init__(use_locking, name)\n","        self._stepsize = stepsize\n","        self._etaplus = etaplus\n","        self._etaminus = etaminus\n","        self._stepsizemax = stepsizemax\n","        self._stepsizemin = stepsizemin\n","\n","    def _create_slots(self, var_list):\n","        '''\n","        :param var_list:\n","        :return:\n","        '''\n","        # Create the beta1 and beta2 accumulators on the same device as the first\n","        # variable.\n","\n","        # Create slots for the first and second moments.\n","        for v in var_list:\n","            self._get_or_make_slot(\n","                v,\n","                tf.ones([v.get_shape().num_elements()], dtype=tf.float32) *\n","                self._stepsize,\n","                \"step\",\n","                self._name, )\n","            self._get_or_make_slot(\n","                v,\n","                tf.zeros([v.get_shape().num_elements()], dtype=tf.float32),\n","                \"delta\",\n","                self._name, )\n","            self._get_or_make_slot(\n","                v,\n","                tf.zeros([v.get_shape().num_elements()], dtype=tf.float32),\n","                \"grad\",\n","                self._name, )\n","\n","    def _apply_dense(self, grad, var):\n","        grad_slot = self.get_slot(var, \"grad\")\n","        step_slot = self.get_slot(var, \"step\")\n","        delta_slot = self.get_slot(var, \"delta\")\n","\n","        grad = tf.reshape(grad, [-1])\n","        sign = tf.cast(tf.sign(grad_slot * grad), tf.int64)\n","        with tf.control_dependencies([sign]):\n","            grad = grad_slot.assign(grad)\n","\n","            p_indices = tf.where(tf.equal(sign, 1))  # positive indices\n","            m_indices = tf.where(tf.equal(sign, -1))  # minus indices\n","            z_indices = tf.where(tf.equal(sign, 0))  # zero indices\n","\n","        step_p_update = tf.expand_dims(\n","            tf.minimum(\n","                tf.gather_nd(step_slot, p_indices) * self._etaplus,\n","                self._stepsizemax), 1)\n","        step_m_update = tf.expand_dims(\n","            tf.maximum(\n","                tf.gather_nd(step_slot, m_indices) * self._etaminus,\n","                self._stepsizemin), 1)\n","        step_z_update = tf.expand_dims(tf.gather_nd(step_slot, z_indices), 1)\n","        with tf.control_dependencies(\n","            [step_p_update, step_m_update, step_z_update]):\n","            step = tf.scatter_update(step_slot, p_indices, step_p_update)\n","            step = tf.scatter_update(step, m_indices, step_m_update)\n","            step = tf.scatter_update(step, z_indices, step_z_update)\n","            step = step_slot.assign(step)\n","\n","        delta_p_update = tf.expand_dims(\n","            tf.gather_nd(tf.sign(grad) * step, p_indices), 1)\n","        delta_z_update = tf.expand_dims(\n","            tf.gather_nd(tf.sign(grad) * step, z_indices), 1)\n","        with tf.control_dependencies([delta_p_update, delta_z_update]):\n","            delta = tf.scatter_update(delta_slot, p_indices, delta_p_update)\n","            delta = tf.scatter_update(delta, z_indices, delta_z_update)\n","            delta = delta_slot.assign(delta)\n","\n","        with tf.control_dependencies([sign]):\n","            grad = tf.scatter_update(grad, m_indices,\n","                                     tf.zeros_like(m_indices, tf.float32))\n","            grad = grad_slot.assign(grad)\n","\n","        up = tf.reshape(delta, var.get_shape())\n","        var_update = var.assign_sub(up, use_locking=self._use_locking)\n","\n","        return tf.group(*[var_update, step, delta, grad])\n","\n","    def _apply_sparse(self, grad, var):\n","        raise NotImplementedError(\"RProp should be used only in batch_mode.\")"],"execution_count":9,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"8TUC4UBbZp0U","colab_type":"text"},"source":["base-train.py"]},{"cell_type":"code","metadata":{"id":"K3EbRFMAZwYI","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600754489596,"user_tz":240,"elapsed":833,"user":{"displayName":"Haiqi Zhu","photoUrl":"","userId":"00612599187227358197"}}},"source":["import math\n","\n","import tensorflow as tf\n","\n","\n","def cosine_decay_learning_rate(learning_rate,\n","                               global_step,\n","                               decay_steps=200,\n","                               alpha=0.01):\n","    # tensorflow==1.4.0에서 못쓰니까 구현.\n","    global_step = tf.cast(global_step, tf.int64)\n","    step = tf.cast(tf.mod(global_step, decay_steps), tf.float32)\n","    cosine_decay = 0.5 * (1.0 + tf.cos(math.pi * step / decay_steps))\n","    decayed = (1 - alpha) * cosine_decay + alpha\n","    return learning_rate * decayed"],"execution_count":10,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"eWt5JBndZ7pv","colab_type":"text"},"source":["local.py"]},{"cell_type":"code","metadata":{"id":"37ZOEdLcZ9xq","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600754545815,"user_tz":240,"elapsed":794,"user":{"displayName":"Haiqi Zhu","photoUrl":"","userId":"00612599187227358197"}}},"source":["import time\n","import math\n","\n","import numpy as np\n","\n","\n","\n","class LocalModel:\n","    def __init__(self, session, models, anchor_idx, anchor_manager,\n","                 batch_manager):\n","        self.session = session\n","        self.models = models\n","        self.batch_manager = batch_manager\n","        self.anchor_idx = anchor_idx\n","        self.anchor_manager = anchor_manager\n","\n","        print('>> update k in anchor_idx [{}].'.format(anchor_idx))\n","        self.train_k = anchor_manager.get_train_k(anchor_idx)\n","        self.valid_k = anchor_manager.get_valid_k(anchor_idx)\n","        self.test_k = anchor_manager.get_test_k(anchor_idx)"],"execution_count":11,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"JjbI3EMBaAJT","colab_type":"text"},"source":["model.py\n"]},{"cell_type":"code","metadata":{"id":"m9nu7eUGaAan","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600754638604,"user_tz":240,"elapsed":777,"user":{"displayName":"Haiqi Zhu","photoUrl":"","userId":"00612599187227358197"}}},"source":["import math\n","\n","import tensorflow as tf\n","\n","import numpy as np\n","\n","\n","\n","def _create_p_or_q_variable(n, rank, batch_manager):\n","    mu = batch_manager.mu\n","    std = batch_manager.std\n","\n","    _mu = math.sqrt(mu / rank)\n","    _std = math.sqrt((math.sqrt(mu * mu + std * std) - mu) / rank)\n","    return tf.Variable(\n","        tf.truncated_normal([n, rank], _mu, _std, dtype=tf.float64))\n","\n","\n","def init_models_for_pre_train(batch_manager):\n","    n_row, n_col = batch_manager.n_user, batch_manager.n_item\n","\n","    u = tf.placeholder(tf.int64, [None], name='u')\n","    i = tf.placeholder(tf.int64, [None], name='i')\n","    r = tf.placeholder(tf.float64, [None], name='r')\n","\n","    # init weights\n","    mu = batch_manager.mu\n","    std = batch_manager.std\n","    p = _create_p_or_q_variable(n_row, PRE_RANK, batch_manager)\n","    q = _create_p_or_q_variable(n_col, PRE_RANK, batch_manager)\n","\n","    p_lookup = tf.nn.embedding_lookup(p, u)\n","    q_lookup = tf.nn.embedding_lookup(q, i)\n","    r_hat = tf.reduce_sum(tf.multiply(p_lookup, q_lookup), 1)\n","\n","    reg_loss = tf.add_n(\n","        [tf.reduce_sum(tf.square(p)),\n","         tf.reduce_sum(tf.square(q))])\n","    loss = tf.reduce_sum(tf.square(r - r_hat)) + PRE_LAMBDA * reg_loss\n","    rmse = tf.sqrt(tf.reduce_mean(tf.square(r - r_hat)))\n","\n","    optimizer = tf.train.MomentumOptimizer(PRE_LEARNING_RATE, 0.9)\n","    # optimizer = tf.train.GradientDescentOptimizer(PRE_LEARNING_RATE)\n","    train_ops = [\n","        optimizer.minimize(loss, var_list=[p]),\n","        optimizer.minimize(loss, var_list=[q])\n","    ]\n","\n","    return {\n","        'u': u,\n","        'i': i,\n","        'r': r,\n","        'train_ops': train_ops,\n","        'loss': loss,\n","        'rmse': rmse,\n","        'p': p,\n","        'q': q,\n","    }\n","\n","\n","def _get_train_op(optimizer, loss, var_list):\n","    gvs = optimizer.compute_gradients(loss, var_list=var_list)\n","    # capped_gvs = [(tf.clip_by_value(grad, -100.0, 100.0), var)\n","    #               for grad, var in gvs]\n","    capped_gvs = gvs\n","    train_op = optimizer.apply_gradients(capped_gvs)\n","    return train_op\n","\n","\n","def init_models(batch_manager):\n","    n_row, n_col = batch_manager.n_user, batch_manager.n_item\n","\n","    u = tf.placeholder(tf.int64, [None], name='u')\n","    i = tf.placeholder(tf.int64, [None], name='i')\n","    r = tf.placeholder(tf.float64, [None], name='r')\n","    k = tf.placeholder(tf.float64, [None, N_ANCHOR], name='k')\n","    k_sum = tf.reduce_sum(k, axis=1)\n","\n","    # init weights\n","    ps, qs, losses, r_hats = [], [], [], []\n","    for anchor_idx in range(N_ANCHOR):\n","        p = _create_p_or_q_variable(n_row, RANK, batch_manager)\n","        q = _create_p_or_q_variable(n_col, RANK, batch_manager)\n","        ps.append(p)\n","        qs.append(q)\n","\n","        p_lookup = tf.nn.embedding_lookup(p, u)\n","        q_lookup = tf.nn.embedding_lookup(q, i)\n","        r_hat = tf.reduce_sum(tf.multiply(p_lookup, q_lookup), axis=1)\n","        r_hats.append(r_hat)\n","\n","    r_hat = tf.reduce_sum(tf.multiply(k, tf.stack(r_hats, axis=1)), axis=1)\n","    r_hat = tf.where(tf.greater(k_sum, 1e-2), r_hat, tf.ones_like(r_hat) * 3)\n","    rmse = tf.sqrt(tf.reduce_mean(tf.square(r - r_hat)))\n","\n","    optimizer = tf.train.GradientDescentOptimizer(LEARNING_RATE)\n","    loss = tf.reduce_sum(tf.square(r_hat - r)) + LAMBDA * tf.reduce_sum(\n","        [tf.reduce_sum(tf.square(p_or_q)) for p_or_q in ps + qs])\n","    train_ops = [\n","        _get_train_op(optimizer, loss, [p, q]) for p, q in zip(ps, qs)\n","    ]\n","\n","    return {\n","        'u': u,\n","        'i': i,\n","        'r': r,\n","        'k': k,\n","        'train_ops': train_ops,\n","        'rmse': rmse,\n","    }"],"execution_count":12,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"Ffynmz8VaXZQ","colab_type":"text"},"source":["pre_trainer.py"]},{"cell_type":"code","metadata":{"id":"eMcLL9ahaXyO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600754690647,"user_tz":240,"elapsed":797,"user":{"displayName":"Haiqi Zhu","photoUrl":"","userId":"00612599187227358197"}}},"source":["import os\n","import time\n","import math\n","import random\n","\n","import tensorflow as tf\n","import numpy as np\n","\n","\n","\n","def _validate(session, batch_manager, models):\n","    valid_rmse = session.run(\n","        models['rmse'],\n","        feed_dict={\n","            models['u']: batch_manager.valid_data[:, 0],\n","            models['i']: batch_manager.valid_data[:, 1],\n","            models['r']: batch_manager.valid_data[:, 2]\n","        })\n","\n","    test_rmse = session.run(\n","        models['rmse'],\n","        feed_dict={\n","            models['u']: batch_manager.test_data[:, 0],\n","            models['i']: batch_manager.test_data[:, 1],\n","            models['r']: batch_manager.test_data[:, 2]\n","        })\n","\n","    return valid_rmse, test_rmse\n","\n","\n","def get_p_and_q(kind, use_cache=True):\n","    if use_cache:\n","        try:\n","            p = np.load('llorma_g/{}-p.npy'.format(kind))\n","            q = np.load('llorma_g/{}-q.npy'.format(kind))\n","            return p, q\n","        except:\n","            print('>> There is no cached p and q.')\n","\n","    batch_manager = BatchManager(kind)\n","    models = init_models_for_pre_train(batch_manager)\n","\n","    session = tf.Session()\n","    session.run(tf.global_variables_initializer())\n","\n","    min_valid_rmse = float(\"Inf\")\n","    min_valid_iter = 0\n","    final_test_rmse = float(\"Inf\")\n","\n","    random_model_idx = random.randint(0, 1000000)\n","\n","    file_path = \"tmp/model-{}.ckpt\".format(random_model_idx)\n","\n","    u = batch_manager.train_data[:, 0]\n","    i = batch_manager.train_data[:, 1]\n","    r = batch_manager.train_data[:, 2]\n","\n","    saver = tf.train.Saver()\n","    for iter in range(1000000):\n","        for train_op in models['train_ops']:\n","            _, loss, train_rmse = session.run(\n","                (train_op, models['loss'], models['rmse']),\n","                feed_dict={models['u']: u,\n","                           models['i']: i,\n","                           models['r']: r})\n","\n","        valid_rmse, test_rmse = _validate(session, batch_manager, models)\n","\n","        if valid_rmse < min_valid_rmse:\n","            min_valid_rmse = valid_rmse\n","            min_valid_iter = iter\n","            final_test_rmse = test_rmse\n","            saver.save(session, file_path)\n","\n","        if iter >= min_valid_iter + 100:\n","            break\n","\n","        print('>> ITER:',\n","              \"{:3d}\".format(iter), \"{:3f}, {:3f} {:3f} / {:3f}\".format(\n","                  train_rmse, valid_rmse, test_rmse, final_test_rmse))\n","\n","    saver.restore(session, file_path)\n","    p, q = session.run(\n","        (models['p'], models['q']),\n","        feed_dict={\n","            models['u']: batch_manager.train_data[:, 0],\n","            models['i']: batch_manager.train_data[:, 1],\n","            models['r']: batch_manager.train_data[:, 2]\n","        })\n","    np.save('llorma_g/{}-p.npy'.format(kind), p)\n","    np.save('llorma_g/{}-q.npy'.format(kind), q)\n","\n","    session.close()\n","    return p, q"],"execution_count":13,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"D7HBMJBFajx1","colab_type":"text"},"source":["trainer.py"]},{"cell_type":"code","metadata":{"id":"suthTDTsakGu","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1600754760566,"user_tz":240,"elapsed":785,"user":{"displayName":"Haiqi Zhu","photoUrl":"","userId":"00612599187227358197"}}},"source":["import os\n","import time\n","import math\n","import random\n","\n","import tensorflow as tf\n","import numpy as np\n","\n","#from . import pre_trainer\n","#from .anchor import AnchorManager\n","#from .batch import BatchManager\n","#from .configs import *\n","#from .local import LocalModel\n","#from .model import init_models\n","\n","\n","def __init_session():\n","    # gpu_options = tf.GPUOptions(\n","    #     per_process_gpu_memory_fraction=GPU_MEMORY_FRAC)\n","    # gpu_config = tf.ConfigProto(gpu_options=gpu_options)\n","    # session = tf.Session(config=gpu_config)\n","\n","    config = tf.ConfigProto()\n","    config.gpu_options.allow_growth = True\n","\n","    session = tf.Session(config=config)\n","    session.run(tf.global_variables_initializer())\n","    return session\n","\n","\n","def _get_k(local_models, kind='train'):\n","    k = np.stack(\n","        [\n","            getattr(local_model, '{}_k'.format(kind))\n","            for local_model in local_models\n","        ],\n","        axis=1)\n","    k = np.clip(k, 0.0, 1.0)\n","    k = np.divide(k, np.sum(k, axis=1, keepdims=1))\n","    k[np.isnan(k)] = 0\n","    return k\n","\n","\n","def _validate(\n","        session,\n","        models,\n","        batch_manager,\n","        valid_k,\n","        test_k, ):\n","    valid_rmse = session.run(\n","        models['rmse'],\n","        feed_dict={\n","            models['u']: batch_manager.valid_data[:, 0],\n","            models['i']: batch_manager.valid_data[:, 1],\n","            models['r']: batch_manager.valid_data[:, 2],\n","            models['k']: valid_k,\n","        })\n","\n","    test_rmse = session.run(\n","        models['rmse'],\n","        feed_dict={\n","            models['u']: batch_manager.test_data[:, 0],\n","            models['i']: batch_manager.test_data[:, 1],\n","            models['r']: batch_manager.test_data[:, 2],\n","            models['k']: test_k,\n","        })\n","\n","    return valid_rmse, test_rmse\n","\n","\n","def _train(kind):\n","    row_latent_init, col_latent_init = pre_trainer.get_p_and_q(\n","        kind, use_cache=USE_CACHE)\n","\n","    batch_manager = BatchManager(kind)\n","    models = init_models(batch_manager)\n","\n","    session = __init_session()\n","    anchor_manager = AnchorManager(\n","        session,\n","        models,\n","        batch_manager,\n","        row_latent_init,\n","        col_latent_init, )\n","    local_models = [\n","        LocalModel(session, models, anchor_idx, anchor_manager, batch_manager)\n","        for anchor_idx in range(N_ANCHOR)\n","    ]\n","\n","    train_k = _get_k(local_models, kind='train')\n","    valid_k = _get_k(local_models, kind='valid')\n","    test_k = _get_k(local_models, kind='test')\n","\n","    min_valid_rmse = float(\"Inf\")\n","    min_valid_iter = 0\n","    final_test_rmse = float(\"Inf\")\n","    start_time = time.time()\n","\n","    batch_rmses = []\n","    train_data = batch_manager.train_data\n","\n","    for iter in range(10000000):\n","        for m in range(0, train_data.shape[0], BATCH_SIZE):\n","            end_m = min(m + BATCH_SIZE, train_data.shape[0])\n","            u = train_data[m:end_m, 0]\n","            i = train_data[m:end_m, 1]\n","            r = train_data[m:end_m, 2]\n","            k = train_k[m:end_m, :]\n","            results = session.run(\n","                [models['rmse']] + models['train_ops'],\n","                feed_dict={\n","                    models['u']: u,\n","                    models['i']: i,\n","                    models['r']: r,\n","                    models['k']: k,\n","                })\n","            batch_rmses.append(results[0])\n","\n","            if m % (BATCH_SIZE * 100) == 0:\n","                print('  - ', results[:1])\n","\n","        if iter % 1 == 0:\n","            valid_rmse, test_rmse = _validate(session, models, batch_manager,\n","                                              valid_k, test_k)\n","            if valid_rmse < min_valid_rmse:\n","                min_valid_rmse = valid_rmse\n","                min_valid_iter = iter\n","                final_test_rmse = test_rmse\n","\n","            batch_rmse = sum(batch_rmses) / len(batch_rmses)\n","            batch_rmses = []\n","            print('  - ITER{:4d}:'.format(iter),\n","                  \"{:.5f}, {:.5f} {:.5f} / {:.5f}\".format(\n","                      batch_rmse, valid_rmse, test_rmse, final_test_rmse))\n","\n","\n","def main(kind):\n","    _train(kind)"],"execution_count":14,"outputs":[]},{"cell_type":"markdown","metadata":{"id":"wC_vmLu6ZQqb","colab_type":"text"},"source":["base/memory_saving_gradients.py"]},{"cell_type":"code","metadata":{"id":"ZA_GZT97bA7X","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":109},"executionInfo":{"status":"ok","timestamp":1600754829255,"user_tz":240,"elapsed":4377,"user":{"displayName":"Haiqi Zhu","photoUrl":"","userId":"00612599187227358197"}},"outputId":"79d4a19f-5d66-4168-a271-1ce198dac95d"},"source":["!pip install toposort"],"execution_count":16,"outputs":[{"output_type":"stream","text":["Collecting toposort\n","  Downloading https://files.pythonhosted.org/packages/e9/8a/321cd8ea5f4a22a06e3ba30ef31ec33bea11a3443eeb1d89807640ee6ed4/toposort-1.5-py2.py3-none-any.whl\n","Installing collected packages: toposort\n","Successfully installed toposort-1.5\n"],"name":"stdout"}]},{"cell_type":"code","metadata":{"id":"YrlWtmVTZS75","colab_type":"code","colab":{"base_uri":"https://localhost:8080/","height":389},"executionInfo":{"status":"error","timestamp":1600754832927,"user_tz":240,"elapsed":1920,"user":{"displayName":"Haiqi Zhu","photoUrl":"","userId":"00612599187227358197"}},"outputId":"092b24af-e2d2-44ab-ad49-9188ba3eb7cd"},"source":["\n","from toposort import toposort\n","import contextlib\n","import numpy as np\n","import tensorflow as tf\n","import tensorflow.contrib.graph_editor as ge\n","import time\n","import sys\n","sys.setrecursionlimit(10000)\n","# refers back to current module if we decide to split helpers out\n","util = sys.modules[__name__]\n","\n","# getting rid of \"WARNING:tensorflow:VARIABLES collection name is deprecated\"\n","setattr(tf.GraphKeys, \"VARIABLES\", \"variables\")\n","\n","# save original gradients since tf.gradient could be monkey-patched to point\n","# to our version\n","from tensorflow.python.ops import gradients as tf_gradients_lib\n","tf_gradients = tf_gradients_lib.gradients\n","\n","MIN_CHECKPOINT_NODE_SIZE=1024    # use lower value during testing\n","\n","# specific versions we can use to do process-wide replacement of tf.gradients\n","def gradients_speed(ys, xs, grad_ys=None, **kwargs):\n","    return gradients(ys, xs, grad_ys, checkpoints='speed', **kwargs)\n","\n","def gradients_memory(ys, xs, grad_ys=None, **kwargs):\n","    return gradients(ys, xs, grad_ys, checkpoints='memory', **kwargs)\n","\n","def gradients_collection(ys, xs, grad_ys=None, **kwargs):\n","    return gradients(ys, xs, grad_ys, checkpoints='collection', **kwargs)\n","\n","def gradients(ys, xs, grad_ys=None, checkpoints='collection', **kwargs):\n","    '''\n","    Authors: Tim Salimans & Yaroslav Bulatov\n","    memory efficient gradient implementation inspired by \"Training Deep Nets with Sublinear Memory Cost\"\n","    by Chen et al. 2016 (https://arxiv.org/abs/1604.06174)\n","    ys,xs,grad_ys,kwargs are the arguments to standard tensorflow tf.gradients\n","    (https://www.tensorflow.org/versions/r0.12/api_docs/python/train.html#gradients)\n","    'checkpoints' can either be\n","        - a list consisting of tensors from the forward pass of the neural net\n","          that we should re-use when calculating the gradients in the backward pass\n","          all other tensors that do not appear in this list will be re-computed\n","        - a string specifying how this list should be determined. currently we support\n","            - 'speed':  checkpoint all outputs of convolutions and matmuls. these ops are usually the most expensive,\n","                        so checkpointing them maximizes the running speed\n","                        (this is a good option if nonlinearities, concats, batchnorms, etc are taking up a lot of memory)\n","            - 'memory': try to minimize the memory usage\n","                        (currently using a very simple strategy that identifies a number of bottleneck tensors in the graph to checkpoint)\n","            - 'collection': look for a tensorflow collection named 'checkpoints', which holds the tensors to checkpoint\n","    '''\n","\n","    #    print(\"Calling memsaving gradients with\", checkpoints)\n","    if not isinstance(ys,list):\n","        ys = [ys]\n","    if not isinstance(xs,list):\n","        xs = [xs]\n","\n","    bwd_ops = ge.get_backward_walk_ops([y.op for y in ys],\n","                                       inclusive=True)\n","\n","    debug_print(\"bwd_ops: %s\", bwd_ops)\n","\n","    # forward ops are all ops that are candidates for recomputation\n","    fwd_ops = ge.get_forward_walk_ops([x.op for x in xs],\n","                                      inclusive=True,\n","                                      within_ops=bwd_ops)\n","    debug_print(\"fwd_ops: %s\", fwd_ops)\n","\n","    # exclude ops with no inputs\n","    fwd_ops = [op for op in fwd_ops if op.inputs]\n","\n","    # don't recompute xs, remove variables\n","    xs_ops = _to_ops(xs)\n","    fwd_ops = [op for op in fwd_ops if not op in xs_ops]\n","    fwd_ops = [op for op in fwd_ops if not '/assign' in op.name]\n","    fwd_ops = [op for op in fwd_ops if not '/Assign' in op.name]\n","    fwd_ops = [op for op in fwd_ops if not '/read' in op.name]\n","    ts_all = ge.filter_ts(fwd_ops, True) # get the tensors\n","    ts_all = [t for t in ts_all if '/read' not in t.name]\n","    ts_all = set(ts_all) - set(xs) - set(ys)\n","\n","    # construct list of tensors to checkpoint during forward pass, if not\n","    # given as input\n","    if type(checkpoints) is not list:\n","        if checkpoints == 'collection':\n","            checkpoints = tf.get_collection('checkpoints')\n","\n","        elif checkpoints == 'speed':\n","            # checkpoint all expensive ops to maximize running speed\n","            checkpoints = ge.filter_ts_from_regex(fwd_ops, 'conv2d|Conv|MatMul')\n","\n","        elif checkpoints == 'memory':\n","\n","            # remove very small tensors and some weird ops\n","            def fixdims(t): # tf.Dimension values are not compatible with int, convert manually\n","                try:\n","                    return [int(e if e.value is not None else 64) for e in t]\n","                except:\n","                    return [0]  # unknown shape\n","            ts_all = [t for t in ts_all if np.prod(fixdims(t.shape)) > MIN_CHECKPOINT_NODE_SIZE]\n","            ts_all = [t for t in ts_all if 'L2Loss' not in t.name]\n","            ts_all = [t for t in ts_all if 'entropy' not in t.name]\n","            ts_all = [t for t in ts_all if 'FusedBatchNorm' not in t.name]\n","            ts_all = [t for t in ts_all if 'Switch' not in t.name]\n","            ts_all = [t for t in ts_all if 'dropout' not in t.name]\n","            # DV: FP16_FIX - need to add 'Cast' layer here to make it work for FP16\n","            ts_all = [t for t in ts_all if 'Cast' not in t.name]\n","\n","            # filter out all tensors that are inputs of the backward graph\n","            with util.capture_ops() as bwd_ops:\n","                tf_gradients(ys, xs, grad_ys, **kwargs)\n","\n","            bwd_inputs = [t for op in bwd_ops for t in op.inputs]\n","            # list of tensors in forward graph that is in input to bwd graph\n","            ts_filtered = list(set(bwd_inputs).intersection(ts_all))\n","            debug_print(\"Using tensors %s\", ts_filtered)\n","\n","            # try two slightly different ways of getting bottlenecks tensors\n","            # to checkpoint\n","            for ts in [ts_filtered, ts_all]:\n","\n","                # get all bottlenecks in the graph\n","                bottleneck_ts = []\n","                for t in ts:\n","                    b = set(ge.get_backward_walk_ops(t.op, inclusive=True, within_ops=fwd_ops))\n","                    f = set(ge.get_forward_walk_ops(t.op, inclusive=False, within_ops=fwd_ops))\n","                    # check that there are not shortcuts\n","                    b_inp = set([inp for op in b for inp in op.inputs]).intersection(ts_all)\n","                    f_inp = set([inp for op in f for inp in op.inputs]).intersection(ts_all)\n","                    if not set(b_inp).intersection(f_inp) and len(b_inp)+len(f_inp) >= len(ts_all):\n","                        bottleneck_ts.append(t)  # we have a bottleneck!\n","                    else:\n","                        debug_print(\"Rejected bottleneck candidate and ops %s\", [t] + list(set(ts_all) - set(b_inp) - set(f_inp)))\n","\n","                # success? or try again without filtering?\n","                if len(bottleneck_ts) >= np.sqrt(len(ts_filtered)): # yes, enough bottlenecks found!\n","                    break\n","\n","            if not bottleneck_ts:\n","                raise Exception('unable to find bottleneck tensors! please provide checkpoint nodes manually, or use checkpoints=\"speed\".')\n","\n","            # sort the bottlenecks\n","            bottlenecks_sorted_lists = tf_toposort(bottleneck_ts, within_ops=fwd_ops)\n","            sorted_bottlenecks = [t for ts in bottlenecks_sorted_lists for t in ts]\n","\n","            # save an approximately optimal number ~ sqrt(N)\n","            N = len(ts_filtered)\n","            if len(bottleneck_ts) <= np.ceil(np.sqrt(N)):\n","                checkpoints = sorted_bottlenecks\n","            else:\n","                step = int(np.ceil(len(bottleneck_ts) / np.sqrt(N)))\n","                checkpoints = sorted_bottlenecks[step::step]\n","\n","        else:\n","            raise Exception('%s is unsupported input for \"checkpoints\"' % (checkpoints,))\n","\n","    checkpoints = list(set(checkpoints).intersection(ts_all))\n","\n","    # at this point automatic selection happened and checkpoints is list of nodes\n","    assert isinstance(checkpoints, list)\n","\n","    debug_print(\"Checkpoint nodes used: %s\", checkpoints)\n","    # better error handling of special cases\n","    # xs are already handled as checkpoint nodes, so no need to include them\n","    xs_intersect_checkpoints = set(xs).intersection(set(checkpoints))\n","    if xs_intersect_checkpoints:\n","        debug_print(\"Warning, some input nodes are also checkpoint nodes: %s\",\n","                    xs_intersect_checkpoints)\n","    ys_intersect_checkpoints = set(ys).intersection(set(checkpoints))\n","    debug_print(\"ys: %s, checkpoints: %s, intersect: %s\", ys, checkpoints,\n","                ys_intersect_checkpoints)\n","    # saving an output node (ys) gives no benefit in memory while creating\n","    # new edge cases, exclude them\n","    if ys_intersect_checkpoints:\n","        debug_print(\"Warning, some output nodes are also checkpoints nodes: %s\",\n","              format_ops(ys_intersect_checkpoints))\n","\n","    # remove initial and terminal nodes from checkpoints list if present\n","    checkpoints = list(set(checkpoints) - set(ys) - set(xs))\n","\n","    # check that we have some nodes to checkpoint\n","    if not checkpoints:\n","        raise Exception('no checkpoints nodes found or given as input! ')\n","\n","    # disconnect dependencies between checkpointed tensors\n","    checkpoints_disconnected = {}\n","    for x in checkpoints:\n","        if x.op and x.op.name is not None:\n","            grad_node = tf.stop_gradient(x, name=x.op.name+\"_sg\")\n","        else:\n","            grad_node = tf.stop_gradient(x)\n","        checkpoints_disconnected[x] = grad_node\n","\n","    # partial derivatives to the checkpointed tensors and xs\n","    ops_to_copy = fast_backward_ops(seed_ops=[y.op for y in ys],\n","                                    stop_at_ts=checkpoints, within_ops=fwd_ops)\n","    debug_print(\"Found %s ops to copy within fwd_ops %s, seed %s, stop_at %s\",\n","                    len(ops_to_copy), fwd_ops, [r.op for r in ys], checkpoints)\n","    debug_print(\"ops_to_copy = %s\", ops_to_copy)\n","    debug_print(\"Processing list %s\", ys)\n","    copied_sgv, info = ge.copy_with_input_replacements(ge.sgv(ops_to_copy), {})\n","    for origin_op, op in info._transformed_ops.items():\n","        op._set_device(origin_op.node_def.device)\n","    copied_ops = info._transformed_ops.values()\n","    debug_print(\"Copied %s to %s\", ops_to_copy, copied_ops)\n","    ge.reroute_ts(checkpoints_disconnected.values(), checkpoints_disconnected.keys(), can_modify=copied_ops)\n","    debug_print(\"Rewired %s in place of %s restricted to %s\",\n","                checkpoints_disconnected.values(), checkpoints_disconnected.keys(), copied_ops)\n","\n","    # get gradients with respect to current boundary + original x's\n","    copied_ys = [info._transformed_ops[y.op]._outputs[0] for y in ys]\n","    boundary = list(checkpoints_disconnected.values())\n","    dv = tf_gradients(ys=copied_ys, xs=boundary+xs, grad_ys=grad_ys, **kwargs)\n","    debug_print(\"Got gradients %s\", dv)\n","    debug_print(\"for %s\", copied_ys)\n","    debug_print(\"with respect to %s\", boundary+xs)\n","\n","    inputs_to_do_before = [y.op for y in ys]\n","    if grad_ys is not None:\n","        inputs_to_do_before += grad_ys\n","    wait_to_do_ops = list(copied_ops) + [g.op for g in dv if g is not None]\n","    my_add_control_inputs(wait_to_do_ops, inputs_to_do_before)\n","\n","    # partial derivatives to the checkpointed nodes\n","    # dictionary of \"node: backprop\" for nodes in the boundary\n","    d_checkpoints = {r: dr for r,dr in zip(checkpoints_disconnected.keys(),\n","                                        dv[:len(checkpoints_disconnected)])}\n","    # partial derivatives to xs (usually the params of the neural net)\n","    d_xs = dv[len(checkpoints_disconnected):]\n","\n","    # incorporate derivatives flowing through the checkpointed nodes\n","    checkpoints_sorted_lists = tf_toposort(checkpoints, within_ops=fwd_ops)\n","    for ts in checkpoints_sorted_lists[::-1]:\n","        debug_print(\"Processing list %s\", ts)\n","        checkpoints_other = [r for r in checkpoints if r not in ts]\n","        checkpoints_disconnected_other = [checkpoints_disconnected[r] for r in checkpoints_other]\n","\n","        # copy part of the graph below current checkpoint node, stopping at\n","        # other checkpoints nodes\n","        ops_to_copy = fast_backward_ops(within_ops=fwd_ops, seed_ops=[r.op for r in ts], stop_at_ts=checkpoints_other)\n","        debug_print(\"Found %s ops to copy within %s, seed %s, stop_at %s\",\n","                    len(ops_to_copy), fwd_ops, [r.op for r in ts],\n","                    checkpoints_other)\n","        debug_print(\"ops_to_copy = %s\", ops_to_copy)\n","        if not ops_to_copy: # we're done!\n","            break\n","        copied_sgv, info = ge.copy_with_input_replacements(ge.sgv(ops_to_copy), {})\n","        for origin_op, op in info._transformed_ops.items():\n","            op._set_device(origin_op.node_def.device)\n","        copied_ops = info._transformed_ops.values()\n","        debug_print(\"Copied %s to %s\", ops_to_copy, copied_ops)\n","        ge.reroute_ts(checkpoints_disconnected_other, checkpoints_other, can_modify=copied_ops)\n","        debug_print(\"Rewired %s in place of %s restricted to %s\",\n","                    checkpoints_disconnected_other, checkpoints_other, copied_ops)\n","\n","        # gradient flowing through the checkpointed node\n","        boundary = [info._transformed_ops[r.op]._outputs[0] for r in ts]\n","        substitute_backprops = [d_checkpoints[r] for r in ts]\n","        dv = tf_gradients(boundary,\n","                          checkpoints_disconnected_other+xs,\n","                          grad_ys=substitute_backprops, **kwargs)\n","        debug_print(\"Got gradients %s\", dv)\n","        debug_print(\"for %s\", boundary)\n","        debug_print(\"with respect to %s\", checkpoints_disconnected_other+xs)\n","        debug_print(\"with boundary backprop substitutions %s\", substitute_backprops)\n","\n","        inputs_to_do_before = [d_checkpoints[r].op for r in ts]\n","        wait_to_do_ops = list(copied_ops) + [g.op for g in dv if g is not None]\n","        my_add_control_inputs(wait_to_do_ops, inputs_to_do_before)\n","\n","        # partial derivatives to the checkpointed nodes\n","        for r, dr in zip(checkpoints_other, dv[:len(checkpoints_other)]):\n","            if dr is not None:\n","                if d_checkpoints[r] is None:\n","                    d_checkpoints[r] = dr\n","                else:\n","                    d_checkpoints[r] += dr\n","        def _unsparsify(x):\n","            if not isinstance(x, tf.IndexedSlices):\n","                return x\n","            assert x.dense_shape is not None, \"memory_saving_gradients encountered sparse gradients of unknown shape\"\n","            indices = x.indices\n","            while indices.shape.ndims < x.values.shape.ndims:\n","                indices = tf.expand_dims(indices, -1)\n","            return tf.scatter_nd(indices, x.values, x.dense_shape)\n","\n","        # partial derivatives to xs (usually the params of the neural net)\n","        d_xs_new = dv[len(checkpoints_other):]\n","        for j in range(len(xs)):\n","            if d_xs_new[j] is not None:\n","                if d_xs[j] is None:\n","                    d_xs[j] = _unsparsify(d_xs_new[j])\n","                else:\n","                    d_xs[j] += _unsparsify(d_xs_new[j])\n","\n","\n","    return d_xs\n","\n","def tf_toposort(ts, within_ops=None):\n","    all_ops = ge.get_forward_walk_ops([x.op for x in ts], within_ops=within_ops)\n","\n","    deps = {}\n","    for op in all_ops:\n","        for o in op.outputs:\n","            deps[o] = set(op.inputs)\n","    sorted_ts = toposort(deps)\n","\n","    # only keep the tensors from our original list\n","    ts_sorted_lists = []\n","    for l in sorted_ts:\n","        keep = list(set(l).intersection(ts))\n","        if keep:\n","            ts_sorted_lists.append(keep)\n","\n","    return ts_sorted_lists\n","\n","def fast_backward_ops(within_ops, seed_ops, stop_at_ts):\n","    bwd_ops = set(ge.get_backward_walk_ops(seed_ops, stop_at_ts=stop_at_ts))\n","    ops = bwd_ops.intersection(within_ops).difference([t.op for t in stop_at_ts])\n","    return list(ops)\n","\n","@contextlib.contextmanager\n","def capture_ops():\n","  \"\"\"Decorator to capture ops created in the block.\n","  with capture_ops() as ops:\n","    # create some ops\n","  print(ops) # => prints ops created.\n","  \"\"\"\n","\n","  micros = int(time.time()*10**6)\n","  scope_name = str(micros)\n","  op_list = []\n","  with tf.name_scope(scope_name):\n","    yield op_list\n","\n","  g = tf.get_default_graph()\n","  op_list.extend(ge.select_ops(scope_name+\"/.*\", graph=g))\n","\n","def _to_op(tensor_or_op):\n","  if hasattr(tensor_or_op, \"op\"):\n","    return tensor_or_op.op\n","  return tensor_or_op\n","\n","def _to_ops(iterable):\n","  if not _is_iterable(iterable):\n","    return iterable\n","  return [_to_op(i) for i in iterable]\n","\n","def _is_iterable(o):\n","  try:\n","    _ = iter(o)\n","  except Exception:\n","    return False\n","  return True\n","\n","DEBUG_LOGGING=False\n","def debug_print(s, *args):\n","  \"\"\"Like logger.log, but also replaces all TensorFlow ops/tensors with their\n","  names. Sensitive to value of DEBUG_LOGGING, see enable_debug/disable_debug\n","  Usage:\n","    debug_print(\"see tensors %s for %s\", tensorlist, [1,2,3])\n","  \"\"\"\n","\n","  if DEBUG_LOGGING:\n","    formatted_args = [format_ops(arg) for arg in args]\n","    print(\"DEBUG \"+s % tuple(formatted_args))\n","\n","def format_ops(ops, sort_outputs=True):\n","  \"\"\"Helper method for printing ops. Converts Tensor/Operation op to op.name,\n","  rest to str(op).\"\"\"\n","\n","  if hasattr(ops, '__iter__') and not isinstance(ops, str):\n","    l = [(op.name if hasattr(op, \"name\") else str(op)) for op in ops]\n","    if sort_outputs:\n","      return sorted(l)\n","    return l\n","  else:\n","    return ops.name if hasattr(ops, \"name\") else str(ops)\n","\n","def my_add_control_inputs(wait_to_do_ops, inputs_to_do_before):\n","    for op in wait_to_do_ops:\n","        ci = [i for i in inputs_to_do_before if op.control_inputs is None or i not in op.control_inputs]\n","        ge.add_control_inputs(op, ci)"],"execution_count":17,"outputs":[{"output_type":"error","ename":"ModuleNotFoundError","evalue":"ignored","traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)","\u001b[0;32m<ipython-input-17-b6f0048b65c8>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mnumpy\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mtf\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m \u001b[0;32mimport\u001b[0m \u001b[0mtensorflow\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcontrib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mgraph_editor\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0mge\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0msys\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n","\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow.contrib'","","\u001b[0;31m---------------------------------------------------------------------------\u001b[0;32m\nNOTE: If your import is failing due to a missing package, you can\nmanually install dependencies using either !pip or !apt.\n\nTo view examples of installing some common dependencies, click the\n\"Open Examples\" button below.\n\u001b[0;31m---------------------------------------------------------------------------\u001b[0m\n"]}]}]}