{"nbformat":4,"nbformat_minor":0,"metadata":{"colab":{"name":"spacyTrail.ipynb","provenance":[],"authorship_tag":"ABX9TyPyYyUh9CCTVp2JTgfGDofG"},"kernelspec":{"name":"python3","display_name":"Python 3"}},"cells":[{"cell_type":"code","metadata":{"id":"mxmXTae4_8NO","colab_type":"code","colab":{},"executionInfo":{"status":"ok","timestamp":1598331811991,"user_tz":240,"elapsed":2242,"user":{"displayName":"Haiqi Zhu","photoUrl":"","userId":"00612599187227358197"}}},"source":["\n","from collections import OrderedDict\n","import numpy as np\n","import spacy\n","from spacy.lang.en.stop_words import STOP_WORDS\n","\n","nlp = spacy.load('en_core_web_sm')\n","\n","class TextRank4Keyword():\n","    \"\"\"Extract keywords from text\"\"\"\n","    \n","    def __init__(self):\n","        self.d = 0.85 # damping coefficient, usually is .85\n","        self.min_diff = 1e-5 # convergence threshold\n","        self.steps = 10 # iteration steps\n","        self.node_weight = None # save keywords and its weight\n","\n","    \n","    def set_stopwords(self, stopwords):  \n","        \"\"\"Set stop words\"\"\"\n","        for word in STOP_WORDS.union(set(stopwords)):\n","            lexeme = nlp.vocab[word]\n","            lexeme.is_stop = True\n","    \n","    def sentence_segment(self, doc, candidate_pos, lower):\n","        \"\"\"Store those words only in cadidate_pos\"\"\"\n","        sentences = []\n","        for sent in doc.sents:\n","            selected_words = []\n","            for token in sent:\n","                # Store words only with cadidate POS tag\n","                if token.pos_ in candidate_pos and token.is_stop is False:\n","                    if lower is True:\n","                        selected_words.append(token.text.lower())\n","                    else:\n","                        selected_words.append(token.text)\n","            sentences.append(selected_words)\n","        return sentences\n","        \n","    def get_vocab(self, sentences):\n","        \"\"\"Get all tokens\"\"\"\n","        vocab = OrderedDict()\n","        i = 0\n","        for sentence in sentences:\n","            for word in sentence:\n","                if word not in vocab:\n","                    vocab[word] = i\n","                    i += 1\n","        return vocab\n","    \n","    def get_token_pairs(self, window_size, sentences):\n","        \"\"\"Build token_pairs from windows in sentences\"\"\"\n","        token_pairs = list()\n","        for sentence in sentences:\n","            for i, word in enumerate(sentence):\n","                for j in range(i+1, i+window_size):\n","                    if j >= len(sentence):\n","                        break\n","                    pair = (word, sentence[j])\n","                    if pair not in token_pairs:\n","                        token_pairs.append(pair)\n","        return token_pairs\n","        \n","    def symmetrize(self, a):\n","        return a + a.T - np.diag(a.diagonal())\n","    \n","    def get_matrix(self, vocab, token_pairs):\n","        \"\"\"Get normalized matrix\"\"\"\n","        # Build matrix\n","        vocab_size = len(vocab)\n","        g = np.zeros((vocab_size, vocab_size), dtype='float')\n","        for word1, word2 in token_pairs:\n","            i, j = vocab[word1], vocab[word2]\n","            g[i][j] = 1\n","            \n","        # Get Symmeric matrix\n","        g = self.symmetrize(g)\n","        \n","        # Normalize matrix by column\n","        norm = np.sum(g, axis=0)\n","        g_norm = np.divide(g, norm, where=norm!=0) # this is ignore the 0 element in norm\n","        \n","        return g_norm\n","\n","    \n","    def get_keywords(self, number=10):\n","        \"\"\"Print top number keywords\"\"\"\n","        node_weight = OrderedDict(sorted(self.node_weight.items(), key=lambda t: t[1], reverse=True))\n","        for i, (key, value) in enumerate(node_weight.items()):\n","            print(key + ' - ' + str(value))\n","            if i > number:\n","                break\n","        \n","        \n","    def analyze(self, text, \n","                candidate_pos=['NOUN', 'PROPN'], \n","                window_size=4, lower=False, stopwords=list()):\n","        \"\"\"Main function to analyze text\"\"\"\n","        \n","        # Set stop words\n","        self.set_stopwords(stopwords)\n","        \n","        # Pare text by spaCy\n","        doc = nlp(text)\n","        \n","        # Filter sentences\n","        sentences = self.sentence_segment(doc, candidate_pos, lower) # list of list of words\n","        \n","        # Build vocabulary\n","        vocab = self.get_vocab(sentences)\n","        \n","        # Get token_pairs from windows\n","        token_pairs = self.get_token_pairs(window_size, sentences)\n","        \n","        # Get normalized matrix\n","        g = self.get_matrix(vocab, token_pairs)\n","        \n","        # Initionlization for weight(pagerank value)\n","        pr = np.array([1] * len(vocab))\n","        \n","        # Iteration\n","        previous_pr = 0\n","        for epoch in range(self.steps):\n","            pr = (1-self.d) + self.d * np.dot(g, pr)\n","            if abs(previous_pr - sum(pr))  < self.min_diff:\n","                break\n","            else:\n","                previous_pr = sum(pr)\n","\n","        # Get weight for each node\n","        node_weight = dict()\n","        for word, index in vocab.items():\n","            node_weight[word] = pr[index]\n","        \n","        self.node_weight = node_weight"],"execution_count":1,"outputs":[]}]}